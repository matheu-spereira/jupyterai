{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cc7dda0",
   "metadata": {},
   "source": [
    "# Spark CSV Read Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713951be",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0d6469",
   "metadata": {},
   "source": [
    "This notebook was created by [Jupyter AI](https://github.com/jupyterlab/jupyter-ai) with the following prompt:\n",
    "\n",
    "> /generate demonstre a leitura de um arquivos csv utilizando spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16f3920",
   "metadata": {},
   "source": [
    "This Jupyter notebook demonstrates how to read a CSV file using Apache Spark, starting with setting up the environment by installing necessary libraries and importing required modules. It proceeds to load data from a CSV file into a DataFrame using Spark's read API, followed by inspecting the schema, basic statistics, and sample rows of the loaded data. The notebook also includes an optional section on common data processing tasks such as filtering, aggregation, and transformations, before concluding with saving the processed DataFrame back to a CSV or another format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04935343",
   "metadata": {},
   "source": [
    "## Load Data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb83f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e059d8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV Data Reader\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73323147",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"path/to/your/data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a677f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(header=\"true\", inferSchema=\"true\").csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdc250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07481d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563efac4",
   "metadata": {},
   "source": [
    "## Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2206a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd4aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Read CSV and Inspect Data\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d49e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"./data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b7b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Schema of the DataFrame:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77030b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBasic Statistics (if numerical columns are present):\")\n",
    "try:\n",
    "    df.describe().show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate statistics: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beed907",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample Rows:\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0a068c",
   "metadata": {},
   "source": [
    "## Data Processing (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fde70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as sum_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15f7504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"CSV Data Processing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee45820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path = \"path/to/your/csvfile.csv\"  # Replace with your actual CSV path\n",
    "data_df = spark.read.options(header=True, inferSchema=True).csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f75635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Filter data based on conditions\n",
    "filtered_df = data_df.filter((col(\"column1\") > 5) & (col(\"column2\") == \"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9457f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first few rows of filtered DataFrame\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ee41dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Aggregate data using groupBy and sum functions\n",
    "aggregated_df = data_df.groupBy(\"some_column\").agg(sum_col(col(\"amount\")).alias(\"total_amount\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ec6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the results of aggregation\n",
    "aggregated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b5f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Transformation - Selecting specific columns\n",
    "selected_columns_df = data_df.select(\"column1\", \"column2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d330ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display selected columns DataFrame\n",
    "selected_columns_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8076ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session when done\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ff560f",
   "metadata": {},
   "source": [
    "## Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d9b277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df13051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session if not already created\n",
    "spark = SparkSession.builder.appName(\"Save Processed Data\").getOrCreate() if 'spark' not in locals() else spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b959e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = ...  # Assume processed_df is defined elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff059f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_csv = \"path/to/output/folder/processed_data.csv\"\n",
    "processed_df.write.mode(\"overwrite\").csv(output_path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebfc3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_parquet = \"path/to/output/folder/processed_data.parquet\"\n",
    "processed_df.write.mode(\"overwrite\").parquet(output_path_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_json = \"path/to/output/folder/processed_data.json\"\n",
    "processed_df.write.mode(\"overwrite\").json(output_path_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57330d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_orc = \"path/to/output/folder/processed_data.orc\"\n",
    "processed_df.write.mode(\"overwrite\").orc(output_path_orc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
